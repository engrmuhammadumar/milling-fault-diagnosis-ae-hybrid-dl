{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, LSTM, Bidirectional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths\n",
    "data_paths = {\n",
    "    'Bearing Fault': r'E:\\1 Paper Work\\Cutting Tool Paper\\Dataset\\cutting tool data\\new_data\\CWT\\Bearing Fault Data',\n",
    "    'Gear Fault': r'E:\\1 Paper Work\\Cutting Tool Paper\\Dataset\\cutting tool data\\new_data\\CWT\\Gear Fault Data',\n",
    "    'Tool Fault': r'E:\\1 Paper Work\\Cutting Tool Paper\\Dataset\\cutting tool data\\new_data\\CWT\\Tool Fault Data',\n",
    "    'Normal': r'E:\\1 Paper Work\\Cutting Tool Paper\\Dataset\\cutting tool data\\new_data\\CWT\\Normal Data'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load images and labels\n",
    "def load_images_and_labels(data_paths):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label, path in data_paths.items():\n",
    "        for file in os.listdir(path):\n",
    "            img_path = os.path.join(path, file)\n",
    "            img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224, 224))\n",
    "            img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "images, labels = load_images_and_labels(data_paths)\n",
    "print(len(images))\n",
    "print(len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# One-hot encode the labels\n",
    "labels_one_hot = tf.keras.utils.to_categorical(labels_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ImageDataGenerator for data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels_one_hot, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "\n",
    "# Load the VGG16 model without the top layer\n",
    "vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers of VGG16\n",
    "for layer in vgg_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers on top of VGG16\n",
    "input_layer = Input(shape=(224, 224, 3))\n",
    "x = vgg_base(input_layer)\n",
    "x = Flatten()(x)\n",
    "cnn_feature_model = Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# Extract features for the training and validation sets\n",
    "X_train_features = cnn_feature_model.predict(X_train)\n",
    "X_val_features = cnn_feature_model.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_feature_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the features to fit LSTM input requirements\n",
    "X_train_features = X_train_features.reshape(X_train_features.shape[0], 1, X_train_features.shape[1])\n",
    "X_val_features = X_val_features.reshape(X_val_features.shape[0], 1, X_val_features.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "lstm_input = Input(shape=(1, X_train_features.shape[2]))\n",
    "x = Bidirectional(LSTM(128))(lstm_input)\n",
    "lstm_feature_model = Model(inputs=lstm_input, outputs=x)\n",
    "\n",
    "# Extract temporal features using bidirectional LSTM\n",
    "X_train_lstm_features = lstm_feature_model.predict(X_train_features)\n",
    "X_val_lstm_features = lstm_feature_model.predict(X_val_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_feature_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate(individual):\n",
    "    selected_features = [i for i in range(len(individual)) if individual[i] == 1]\n",
    "    if len(selected_features) == 0:\n",
    "        return 0,\n",
    "    \n",
    "    X_train_selected = X_train_lstm_features[:, selected_features]\n",
    "    X_val_selected = X_val_lstm_features[:, selected_features]\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(64, activation='relu', input_dim=len(selected_features)),\n",
    "        Dense(4, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train_selected, y_train, epochs=10, batch_size=8, verbose=0)\n",
    "    \n",
    "    _, accuracy = model.evaluate(X_val_selected, y_val, verbose=0)\n",
    "    return accuracy,\n",
    "\n",
    "# Define GA components\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=X_train_lstm_features.shape[1])\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "# GA execution\n",
    "population = toolbox.population(n=50)\n",
    "algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=3, verbose=True)\n",
    "\n",
    "# Get the best individual\n",
    "best_ind = tools.selBest(population, k=1)[0]\n",
    "selected_features = [i for i in range(len(best_ind)) if best_ind[i] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features based on GA\n",
    "X_train_selected = X_train_lstm_features[:, selected_features]\n",
    "X_val_selected = X_val_lstm_features[:, selected_features]\n",
    "\n",
    "# Define the final classification model\n",
    "final_model = tf.keras.Sequential([\n",
    "    Dense(64, activation='relu', input_dim=len(selected_features)),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "final_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the final model\n",
    "history = final_model.fit(X_train_selected, y_train, epochs=5, batch_size=8, validation_data=(X_val_selected, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = final_model.evaluate(X_val_selected, y_val)\n",
    "print(f'Validation Accuracy: {val_accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training accuracy values\n",
    "acc = history.history['accuracy']\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and save accuracy\n",
    "plt.plot(history.epoch,history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.epoch,history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1.1])\n",
    "plt.legend(loc='lower right') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Predict labels for the training data\n",
    "y_train_pred = final_model.predict(X_train_selected)\n",
    "y_train_pred_classes = np.argmax(y_train_pred, axis=1)\n",
    "y_train_true_classes = np.argmax(y_train, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_train_true_classes, y_train_pred_classes)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "#plt.title('Confusion Matrix - Training Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "y_val_pred = final_model.predict(X_val_selected)\n",
    "y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
    "y_val_true_classes = np.argmax(y_val, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_val_true_classes, y_val_pred_classes)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(data_paths.keys()))\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "disp.plot(ax=ax, cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Binarize the output\n",
    "y_train_binarized = label_binarize(y_train_true_classes, classes=[0, 1, 2, 3])\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "n_classes = y_train.shape[1]\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_train_binarized[:, i], y_train_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "colors = ['aqua', 'darkorange', 'cornflowerblue', 'green']\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2, label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic - Training Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce features to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_selected)\n",
    "\n",
    "# Plot decision boundaries\n",
    "x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1\n",
    "y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "# Create a mesh to plot points in\n",
    "Z = final_model.predict(pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()]))\n",
    "Z = np.argmax(Z, axis=1)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train_true_classes, edgecolor='k', s=20)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Decision Boundary')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Reduce features to 2D using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_train_tsne = tsne.fit_transform(X_train_selected)\n",
    "\n",
    "# Plot t-SNE\n",
    "plt.figure()\n",
    "for i, color, label in zip(range(n_classes), colors, label_encoder.classes_):\n",
    "    plt.scatter(X_train_tsne[y_train_true_classes == i, 0], X_train_tsne[y_train_true_classes == i, 1], color=color, lw=2, label=label)\n",
    "plt.title('t-SNE')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Generate classification report\n",
    "train_report = classification_report(y_train_true_classes, y_train_pred_classes, target_names=label_encoder.classes_)\n",
    "print(train_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data - replace with your actual feature names or indices if available\n",
    "# Let's assume `selected_features` contains the indices of the features chosen by GA\n",
    "selected_features = [0, 2, 4, 6, 7, 10, 11, 15, 16, 19, 20, 23, 24, 26, 30, 31, 33, 39, 40, 42, 44, 45, 47, \n",
    "                     53, 54, 57, 58, 62, 63, 64, 66, 67, 70, 75, 76, 78, 80, 81, 83, 84, 85, 89, 92, 93, \n",
    "                     95, 96, 97, 99, 101, 103, 104, 107, 108, 110, 111, 112, 114, 115, 116, 118, 120, 126, \n",
    "                     127, 128, 129, 132, 135, 140, 143, 144, 148, 149, 152, 156, 157, 158, 160, 163, 170, \n",
    "                     176, 177, 178, 184, 185, 186, 187, 191, 194, 195, 198, 202, 203, 206, 208, 209, 210, \n",
    "                     213, 215, 218, 220, 222, 223, 226, 227, 229, 230, 232, 237, 238, 243, 244, 245, 246, \n",
    "                     254]\n",
    "\n",
    "# Create a DataFrame for visualization purposes\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Importance': [1] * len(selected_features)  # Assign equal importance to each selected feature\n",
    "})\n",
    "\n",
    "# Sort the features by index or name (optional)\n",
    "feature_importance_df = feature_importance_df.sort_values(by=\"Feature\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], align='center')\n",
    "plt.xlabel('Importance Score (Selected by GA)')\n",
    "plt.title('Important Features Selected by Genetic Algorithm')\n",
    "plt.gca().invert_yaxis()  # Show higher feature indices at the top if needed\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data - Replace with your actual selected feature indices\n",
    "selected_features = [0, 2, 4, 6, 7, 10, 11, 15, 16, 19, 20, 23, 24, 26, 30, 31, 33, 39, 40, 42, 44, 45, 47, \n",
    "                     53, 54, 57, 58, 62, 63, 64, 66, 67, 70, 75, 76, 78, 80, 81, 83, 84, 85, 89, 92, 93, \n",
    "                     95, 96, 97, 99, 101, 103, 104, 107, 108, 110, 111, 112, 114, 115, 116, 118, 120, 126, \n",
    "                     127, 128, 129, 132, 135, 140, 143, 144, 148, 149, 152, 156, 157, 158, 160, 163, 170, \n",
    "                     176, 177, 178, 184, 185, 186, 187, 191, 194, 195, 198, 202, 203, 206, 208, 209, 210, \n",
    "                     213, 215, 218, 220, 222, 223, 226, 227, 229, 230, 232, 237, 238, 243, 244, 245, 246, \n",
    "                     254]\n",
    "\n",
    "# Plot histogram of selected feature indices\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(selected_features, bins=30, edgecolor='black')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Selected Features by Genetic Algorithm')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample weights from the Dense layer (replace with actual weights from your model)\n",
    "# Assuming `dense_weights` is a NumPy array of shape (103,) for each selected feature\n",
    "# You can obtain this array after training the model by using `model.layers[-2].get_weights()[0]`\n",
    "dense_weights = np.random.rand(103)  # Replace with actual Dense layer weights\n",
    "\n",
    "# Calculate feature importance as the absolute value of the Dense layer weights\n",
    "feature_importance = np.abs(dense_weights)\n",
    "\n",
    "# Sort features by importance for a cleaner plot\n",
    "sorted_indices = np.argsort(feature_importance)[::-1]\n",
    "sorted_importance = feature_importance[sorted_indices]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(sorted_importance)), sorted_importance, align='center')\n",
    "plt.xlabel('Feature Index (sorted by importance)')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.title('Feature Importance of 103 Selected Features (based on Dense Layer Weights)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data - Replace with your actual selected feature indices\n",
    "selected_features = [0, 2, 4, 6, 7, 10, 11, 15, 16, 19, 20, 23, 24, 26, 30, 31, 33, 39, 40, 42, 44, 45, 47, \n",
    "                     53, 54, 57, 58, 62, 63, 64, 66, 67, 70, 75, 76, 78, 80, 81, 83, 84, 85, 89, 92, 93, \n",
    "                     95, 96, 97, 99, 101, 103, 104, 107, 108, 110, 111, 112, 114, 115, 116, 118, 120, 126, \n",
    "                     127, 128, 129, 132, 135, 140, 143, 144, 148, 149, 152, 156, 157, 158, 160, 163, 170, \n",
    "                     176, 177, 178, 184, 185, 186, 187, 191, 194, 195, 198, 202, 203, 206, 208, 209, 210, \n",
    "                     213, 215, 218, 220, 222, 223, 226, 227, 229, 230, 232, 237, 238, 243, 244, 245, 246, \n",
    "                     254]\n",
    "\n",
    "# Plot histogram of selected feature indices with enhancements\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(selected_features, bins=30, edgecolor='black', color='steelblue', alpha=0.7)\n",
    "plt.xlabel('Feature Index', fontsize=14, weight='bold')\n",
    "plt.ylabel('Frequency', fontsize=14, weight='bold')\n",
    "#plt.title('Distribution of Selected Features by Genetic Algorithm', fontsize=16, weight='bold')\n",
    "#plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample weights - replace with actual Dense layer weights or feature importance scores\n",
    "dense_weights = np.random.rand(103)  # Replace with actual Dense layer weights\n",
    "feature_importance = np.abs(dense_weights)\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_indices = np.argsort(feature_importance)[::-1]\n",
    "sorted_importance = feature_importance[sorted_indices]\n",
    "\n",
    "# Create an enhanced bar plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.bar(range(len(sorted_importance)), sorted_importance, color=plt.cm.viridis(np.linspace(0, 1, len(sorted_importance))), edgecolor='black', alpha=0.8)\n",
    "plt.xlabel('Feature Index (sorted by importance)', fontsize=14, weight='bold')\n",
    "plt.ylabel('Importance Score', fontsize=14, weight='bold')\n",
    "#plt.title('Feature Importance of 103 Selected Features', fontsize=16, weight='bold')\n",
    "#plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
